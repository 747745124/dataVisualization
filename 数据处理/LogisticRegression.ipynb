{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 逻辑回归完成二分类，利用sklearn库完成tokenize"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   ItemID  label                                              tweet\n",
       "0       1      0                       is so sad for my APL frie...\n",
       "1       2      0                     I missed the New Moon trail...\n",
       "2       3      1                            omg its already 7:30 :O\n",
       "3       4      0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4       5      0           i think mi bf is cheating on me!!!   ..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ItemID</th>\n      <th>label</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>is so sad for my APL frie...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n      <td>I missed the New Moon trail...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>omg its already 7:30 :O</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>i think mi bf is cheating on me!!!   ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import re\n",
    "#数据来源Kaggle https://www.kaggle.com/imrandude/twitter-sentiment-analysis/notebooks\n",
    "#共10万条 \n",
    "df = pd.read_csv('train.csv',encoding='ISO-8859-1')\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "## 数据清洗\n",
    "* 需要完成删除标点符号\n",
    "* 删除at用户内容（采用tweet-preprocessor库）\n",
    "* 统一转小写"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using tweet-preprocessor to clean tweets\n",
    "from string import punctuation\n",
    "import preprocessor as p\n",
    "for i in range(0,len(df)):\n",
    "    df['tweet'][i] = p.clean(df['tweet'][i])\n",
    "    df['tweet'][i] = ''.join([c for c in df['tweet'][i] if c not in punctuation])\n",
    "    df['tweet'][i] = df['tweet'][i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       ItemID  label                                              tweet\n",
       "99984   99996      0  cupcake seems like a repeating problem hope yo...\n",
       "99985   99997      1  cupcake arrrr we both replied to each other ov...\n",
       "99986   99998      0                        cupcake2120 ya i thought so\n",
       "99987   99999      1  cupcakedollie yes yes im glad you had more fun...\n",
       "99988  100000      1                       cupcakekayla haha yes you do"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ItemID</th>\n      <th>label</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>99984</th>\n      <td>99996</td>\n      <td>0</td>\n      <td>cupcake seems like a repeating problem hope yo...</td>\n    </tr>\n    <tr>\n      <th>99985</th>\n      <td>99997</td>\n      <td>1</td>\n      <td>cupcake arrrr we both replied to each other ov...</td>\n    </tr>\n    <tr>\n      <th>99986</th>\n      <td>99998</td>\n      <td>0</td>\n      <td>cupcake2120 ya i thought so</td>\n    </tr>\n    <tr>\n      <th>99987</th>\n      <td>99999</td>\n      <td>1</td>\n      <td>cupcakedollie yes yes im glad you had more fun...</td>\n    </tr>\n    <tr>\n      <th>99988</th>\n      <td>100000</td>\n      <td>1</td>\n      <td>cupcakekayla haha yes you do</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据集划分\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['tweet'], df['label'], test_size=0.3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Logistic Regression to classify\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "predicted = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classification report for classifier LogisticRegression():\n              precision    recall  f1-score   support\n\n           0       0.73      0.68      0.70     12287\n           1       0.79      0.82      0.80     17710\n\n    accuracy                           0.77     29997\n   macro avg       0.76      0.75      0.75     29997\nweighted avg       0.76      0.77      0.76     29997\n\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (lr, classification_report(y_test, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLrModel():\n",
    "    df = pd.read_csv('train.csv')\n",
    "    df.head()\n",
    "\n",
    "    for i in range(0,len(df)):\n",
    "        df['tweet'][i] = p.clean(df['tweet'][i])\n",
    "        df['tweet'][i] = ''.join([c for c in df['tweet'][i] if c not in punctuation])\n",
    "        df['tweet'][i] = df['tweet'][i].lower()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df['tweet'], df['label'], test_size=0.3, shuffle=False)\n",
    "\n",
    "    vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train,y_train)\n",
    "\n",
    "    return [vectorizer,lr]"
   ]
  },
  {
   "source": [
    "## 对于爬虫数据集进行处理"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                    id      conversation_id               created_at  \\\n",
       "0  1344795464877699076  1344793464127578112  2021-01-01 07:59:58 CST   \n",
       "1  1344795454987493376  1344795454987493376  2021-01-01 07:59:56 CST   \n",
       "2  1344795428596936705  1344795428596936705  2021-01-01 07:59:49 CST   \n",
       "3  1344795409550614529  1344795409550614529  2021-01-01 07:59:45 CST   \n",
       "4  1344795404538359811  1344795404538359811  2021-01-01 07:59:44 CST   \n",
       "\n",
       "         date      time  timezone              user_id         username  \\\n",
       "0  2021-01-01  07:59:58       800             20734655        urbanfoxe   \n",
       "1  2021-01-01  07:59:56       800   913589759544401920  stephthacreator   \n",
       "2  2021-01-01  07:59:49       800             15359416       nellbryden   \n",
       "3  2021-01-01  07:59:45       800  1253813929039597569   slowstrokeking   \n",
       "4  2021-01-01  07:59:44       800             50706690            511ny   \n",
       "\n",
       "                   name                                              place  \\\n",
       "0            Jenny Foxe                                                NaN   \n",
       "1               Stephhh                                                NaN   \n",
       "2           Nell Bryden                                                NaN   \n",
       "3  HOUSTON TX jan 22-25                                                NaN   \n",
       "4          511 New York  {'type': 'Point', 'coordinates': [40.750046, -...   \n",
       "\n",
       "   ...                    geo source user_rt_id user_rt retweet_id  \\\n",
       "0  ...  40.75773,-73.9857,5km    NaN        NaN     NaN        NaN   \n",
       "1  ...  40.75773,-73.9857,5km    NaN        NaN     NaN        NaN   \n",
       "2  ...  40.75773,-73.9857,5km    NaN        NaN     NaN        NaN   \n",
       "3  ...  40.75773,-73.9857,5km    NaN        NaN     NaN        NaN   \n",
       "4  ...  40.75773,-73.9857,5km    NaN        NaN     NaN        NaN   \n",
       "\n",
       "                                            reply_to  retweet_date  translate  \\\n",
       "0  [{'screen_name': 'peterc83', 'name': 'Peter', ...           NaN        NaN   \n",
       "1                                                 []           NaN        NaN   \n",
       "2                                                 []           NaN        NaN   \n",
       "3                                                 []           NaN        NaN   \n",
       "4                                                 []           NaN        NaN   \n",
       "\n",
       "  trans_src trans_dest  \n",
       "0       NaN        NaN  \n",
       "1       NaN        NaN  \n",
       "2       NaN        NaN  \n",
       "3       NaN        NaN  \n",
       "4       NaN        NaN  \n",
       "\n",
       "[5 rows x 36 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>conversation_id</th>\n      <th>created_at</th>\n      <th>date</th>\n      <th>time</th>\n      <th>timezone</th>\n      <th>user_id</th>\n      <th>username</th>\n      <th>name</th>\n      <th>place</th>\n      <th>...</th>\n      <th>geo</th>\n      <th>source</th>\n      <th>user_rt_id</th>\n      <th>user_rt</th>\n      <th>retweet_id</th>\n      <th>reply_to</th>\n      <th>retweet_date</th>\n      <th>translate</th>\n      <th>trans_src</th>\n      <th>trans_dest</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1344795464877699076</td>\n      <td>1344793464127578112</td>\n      <td>2021-01-01 07:59:58 CST</td>\n      <td>2021-01-01</td>\n      <td>07:59:58</td>\n      <td>800</td>\n      <td>20734655</td>\n      <td>urbanfoxe</td>\n      <td>Jenny Foxe</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>40.75773,-73.9857,5km</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[{'screen_name': 'peterc83', 'name': 'Peter', ...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1344795454987493376</td>\n      <td>1344795454987493376</td>\n      <td>2021-01-01 07:59:56 CST</td>\n      <td>2021-01-01</td>\n      <td>07:59:56</td>\n      <td>800</td>\n      <td>913589759544401920</td>\n      <td>stephthacreator</td>\n      <td>Stephhh</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>40.75773,-73.9857,5km</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[]</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1344795428596936705</td>\n      <td>1344795428596936705</td>\n      <td>2021-01-01 07:59:49 CST</td>\n      <td>2021-01-01</td>\n      <td>07:59:49</td>\n      <td>800</td>\n      <td>15359416</td>\n      <td>nellbryden</td>\n      <td>Nell Bryden</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>40.75773,-73.9857,5km</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[]</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1344795409550614529</td>\n      <td>1344795409550614529</td>\n      <td>2021-01-01 07:59:45 CST</td>\n      <td>2021-01-01</td>\n      <td>07:59:45</td>\n      <td>800</td>\n      <td>1253813929039597569</td>\n      <td>slowstrokeking</td>\n      <td>HOUSTON TX jan 22-25</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>40.75773,-73.9857,5km</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[]</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1344795404538359811</td>\n      <td>1344795404538359811</td>\n      <td>2021-01-01 07:59:44 CST</td>\n      <td>2021-01-01</td>\n      <td>07:59:44</td>\n      <td>800</td>\n      <td>50706690</td>\n      <td>511ny</td>\n      <td>511 New York</td>\n      <td>{'type': 'Point', 'coordinates': [40.750046, -...</td>\n      <td>...</td>\n      <td>40.75773,-73.9857,5km</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[]</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 36 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "nyc = pd.read_csv('NewYork01.csv')\n",
    "nyc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import preprocessor as p\n",
    "for i in range(0,len(nyc)):\n",
    "    nyc['tweet'][i] = p.clean(nyc['tweet'][i])\n",
    "    nyc['tweet'][i] = ''.join([c for c in nyc['tweet'][i] if c not in punctuation])\n",
    "    nyc['tweet'][i] = nyc['tweet'][i].lower()\n",
    "   \n",
    "vec,lrModel = getLrModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = vec.transform(nyc['tweet'])\n",
    "predict = lrModel.predict(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "no no no this cant be real lmao rt i cannot breathe\nnew opp pack in the air nigga this gas or what capitol at washington dc\nanother soccer lover reposted from times square new york city\nacabou de publicar uma foto em times square new york city\ncleared incident on nb at sts rockefeller ctr\ncleared incident on at sts rockefeller ctr\ncleared incident on at sts rockefeller ctr\ncleared incident on nb at sts rockefeller ctr\nim at in new york ny\nim at tomiz in new york ny\nim at stk steakhouse midtown nyc in new york ny\nim at modern szechuan in new york ny\nim at the red flame in new york ny\nim at hippodrome in new york ny\ncabin pressure beat milky way galaxy\nincident on at sts rockefeller ctr\nincident on nb at sts rockefeller ctr\ntoyota rav4 driver y104060c blocked the crosswalk near th ave on january and has been reported to this is in manhattan community board amp\ncleared incident on sb at st port authority bus terminal\ncleared incident on sb at st port authority bus terminal\nincident on sb at st port authority bus terminal\nno topps chrome in stock currently sorry about that\nmy new project drops features from and will be available on all digital streaming platforms including warner music group\nwere kicking off the new year with off on cards for the month of january some exclusions apply offer cannot be combined with other promotions or applied to previous purchases\nnew comics day picked up shouts to\n606 2279\n"
     ]
    }
   ],
   "source": [
    "for i in range(200,300):\n",
    "    if(predict[i]==0):\n",
    "        print(nyc['tweet'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "606 2279\n"
     ]
    }
   ],
   "source": [
    "cnt0=0#negative\n",
    "cnt1=0#positive\n",
    "for i in range(0,len(nyc)):\n",
    "    if(predict[i]==0):\n",
    "        cnt0+=1\n",
    "    else:\n",
    "        cnt1+=1\n",
    "print(cnt0,cnt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import re\n",
    "from string import punctuation\n",
    "import preprocessor as p\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "\n",
    "def getLrModel():\n",
    "    df = pd.read_csv('train.csv')\n",
    "    df.head()\n",
    "\n",
    "    for i in range(0, len(df)):\n",
    "        df['tweet'][i] = p.clean(df['tweet'][i])\n",
    "        df['tweet'][i] = ''.join(\n",
    "            [c for c in df['tweet'][i] if c not in punctuation])\n",
    "        df['tweet'][i] = df['tweet'][i].lower()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df['tweet'], df['label'], test_size=0.3, shuffle=False)\n",
    "\n",
    "    vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    joblib.dump(lr, 'saved_model/lr.pkl')\n",
    "    joblib.dump(vectorizer, 'saved_model/vec.pkl')\n",
    "    return [vectorizer, lr]\n",
    "\n",
    "\n",
    "def preprocess(fileName):\n",
    "    nyc = pd.read_csv(fileName)\n",
    "    for i in range(0, len(nyc)):\n",
    "        nyc['tweet'][i] = p.clean(nyc['tweet'][i])\n",
    "        nyc['tweet'][i] = ''.join(\n",
    "            [c for c in nyc['tweet'][i] if c not in punctuation])\n",
    "        nyc['tweet'][i] = nyc['tweet'][i].lower()\n",
    "    return nyc\n",
    "\n",
    "\n",
    "def predict(dataFrame, vec, lrModel):\n",
    "    nyc = dataFrame\n",
    "    tweet = vec.transform(nyc['tweet'])\n",
    "    predict = lrModel.predict(tweet)\n",
    "    cnt0 = 0  # negative\n",
    "    cnt1 = 0  # positive\n",
    "    for i in range(0, len(nyc)):\n",
    "        if(predict[i] == 0):\n",
    "            cnt0 += 1\n",
    "        else:\n",
    "            cnt1 += 1\n",
    "    print(cnt0, cnt1)\n",
    "    print(cnt0/(len(nyc)+1), cnt1/(len(nyc)+1))\n",
    "    return [predict, cnt0, cnt1]\n",
    "\n",
    "\n",
    "def allInOne(fileName):\n",
    "    nyc = preprocess(fileName)\n",
    "    lrModel = joblib.load('saved_model/lr.pkl')\n",
    "    vec = joblib.load('saved_model/vec.pkl')\n",
    "    result = predict(nyc, vec, lrModel)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ing at that map and it gets uncomfortable. Very quickly. lol\n",
      "@Kofie @ASPCA He good\n",
      "@kim_hoyos please\n",
      "@mrsrkfj What is this life?!?!\n",
      "@kim_hoyos just @ me next time\n",
      "@DankwaBrooks Oh ok\n",
      "@Kofie You started it!\n",
      "@Kofie I’ll feed him a Kof-a-loaf\n",
      "@yours0ftblood RIGHT LMAOOO like u took the time to give three stars SPECIFICALLY? you thought people would need this information? also “this place looks awesome at night” great thank you for the heads up :)\n",
      "The wages of sin..\n",
      "me: i love open world RPGs where my choices MATTER   also me: thank god for this gamefaqs entry that breaks down in meticulous detail the consequences of all of my potential choices\n",
      "@ianofspencer That's sweeter\n",
      "My husband and I are spending the first night of 2020 rewatching Scream\n",
      "@dan_munz @mikekruger 3 means you could have 3 people sitting uncomfortably close to you. That gives 5 an advantage imo (4 is def most desireable.)\n",
      "@SuziSteffen Thank you! Glad the movie connected with you ❤️\n",
      "@SincerelyBlogg @people Lord.\n",
      "@michelledeidre @RoseParade ❤️❤️❤️❤️\n",
      "@BoneyStarks Correct\n",
      "@anna01930 😉\n",
      "someone else replied “the other review was right, smells like farts” like people really just drive past and say this shit\n",
      "@n_vpatel Now you're all grown up.\n",
      "@BieglerTom You’re making my point about how bad the wildfires are.\n",
      "@srothbell That’s all the stuff I’m doing in 2021\n",
      "@MollyJongFast Cool that the paper of record gave him a ton of free publicity today.\n",
      "Halfway through my You Season 2 binge and I think... a meme just spoiled it for me?? 🤬 #YouNetflix\n",
      "gus fring acts exactly like gus fring in the mandalorian\n",
      "@erikaharwood hi\n",
      "@reelsistas Seriously!\n",
      "“Hey dad” but “dad” is Jay-Z\n",
      "@McBaine146 @noonanjo She could have definitely done cool career stuff after that\n",
      "@Journeys_Film 👀👀👀\n",
      "@WriteinBK Yesssss\n",
      "@CybelDP Omg you survived!\n",
      "@BluePhoenix1 Lol yup!\n",
      "@RustenHurd Hahaha I was just saying she had a lot to lose! She’d accomplished so much\n",
      "@bariweiss @nytopinion Thank you. Lead on!\n",
      "@Kehlani ❤️❤️❤️ praying for you!\n",
      "@InHollywoodland A whole mess\n",
      "@InHollywoodland Yes\n",
      "Can you imagine if you woke up every day and Beyoncé was your mom?\n",
      "I still can’t believe Padme, a senator, gave up her life and career to be with a creepy volatile Justin Bieber-lite Jedi-in-training\n",
      "\"people get catfished off them filters. Not me, but I know people who have\"- Dennis Smith Jr  Lmao\n",
      "@marksbirch It is feeling that way. May need to pivot back into lifeguarding.\n",
      "Just got to see 30 of FDNY's finest storm my building while I was in the lobby thanks to a family leaving plastic in their oven. Happy New Year.\n",
      "@ELGINDOTCOM Nothing compares to that Chicago Bulls game at the Garden\n",
      "@LolaVicious FRFR\n",
      "@JJohnsonLaw @NBCNews She’d be great! But I think someone new and not run through the primary is more likely.\n",
      "@cjsabbagh1 @ssscorvus I love that movie\n",
      "@cjsabbagh1 @ssscorvus I loved the first movie tho\n",
      "@cjsabbagh1 I remember asking this same question at the time of the first movie. Lots of people wrote about very good articles exploring that\n",
      "watching a show on Netflix, thought one of the actors looked vaguely handsome and familiar, turned out to be @maxjenkinsyall on DEAD TO ME.\n",
      "@tiffmoustakas @DohaMadani choosing the right running mate is crucial for 2020.\n",
      "@MegaMuto I see it!\n",
      "@twodigitz23 @CNN Dude I literally said this is exactly how it should be done. I applauded this person!\n",
      "i swear my mother revenge-programmed this with a “melodrama” option. like stuck on cliff? in a brooklyn studio? ok roomba\n",
      "@theferocity FACTS.\n",
      "@gabrielroth I am 7”2 and 4 is still the best for (anti)social reasons\n",
      "@j_acquelines @CURRENTMOODio @oneouncegold @NguyenCoffeeNYC 🍾🍾🍾\n",
      "@LouisPeitzman @SamuelAAdams More seat 4s for me!\n",
      "@SamuelAAdams It’s just hair and headphone it’s fine\n",
      "@SamuelAAdams 4 is by far the best! You can rest your head on the window and sink into a protective shell\n",
      "@birbigs @NewOneBway Consider yourself reminded (albeit 4 days early), Birbigs! Please tell us the joke! @birbigs\n",
      "2021 boutta be the move\n",
      "@DankwaBrooks Sure waa\n",
      "@ScarlettEHarris Oh dear!\n",
      "@thugpop she really ate that 😩\n",
      "\"mind ya business, nigga. Tuck ya paper\"\n",
      "@asarians @CNN I literally said this is how you do it. Read what I said before taking a shot next time.\n",
      "@RotoGut Read that as “OBP”, pretty close\n",
      "Bring it on, 2020.\n",
      "@moorehn @andohehir @AndrewKirell @AndrewHusband @AndrewDesiderio They def will\n",
      "@Nixon_Corral The Carolinas don’t count, but driving from Portland to a DQ parking lot in Vancouver WA for 20 minutes just to make a phone call and check off Washington does.\n",
      "@caitiedelaney Bring a bucket of water from home and set it free at the beach\n",
      "@moorehn @andohehir @AndrewKirell and @AndrewHusband and @AndrewDesiderio belong in here too\n",
      "Sup guys, this is Spooky and I am on my way to @nycnextlevel - Happy New Year everyone! I am not sure who we will see today but I am excited to bring in 2020 with another Next Level Battle Circuit. More info soon when we go live.\n",
      "Just saw Brittany Runs a Marathon. Thanks for making me ugly cry for the start of 2020, @jillianbell.\n",
      "@Vahn16 did you know that ihop's eggs are so good because they mix in PANCAKE BATTER\n",
      "that first nap of 2020 absolutely slapped\n",
      "Ryan Eggold didn't win any awards for his role as Tom Keen, but I loved him in that part more than most other TV roles.\n",
      "@Big5Army @Stelfreeze The curve of the helmet is where he’s showing off. It’s so nice\n",
      "@Big5Army @Stelfreeze Jeez, those lines are beautiful\n",
      "@xackclaret @thebirdprince @ComicBook Now Zach just needs a Switch. The bias is strong 😭\n",
      "Tom Keen went out like a fucking G and a half on @NBCTheBlacklist\n",
      "@NicoleSegniniS @DaniCHurtado_ Thank you my love! All is good now ❤️\n",
      "@ITSDJFLOW Not future lol\n",
      "welcome to my third decade of being furious about that episode of Friends where Monica thinks that Chandler is sexually attracted to sharks.\n",
      "Idk if The Blacklist is my favorite series of the last decade but I rewatch it more than any other show\n",
      "@CNN This is how you do something nice for someone. Don’t film it and post to social. Just do a good thing and be glad you did it. No socia props needed.\n",
      "@JackieKostek @Palms @KTNV This is wonderful. Happy New Year Jackie!\n",
      "@AbbyMCarney What a very specific ask\n",
      "@Kehlani ❤️ sending luv bb\n",
      "It’s January and the sun has been setting since 10 am\n",
      "2020 is gonna be amazing. Why? My birthday and Christmas are on Fridays, and 4th of July and Halloween are on Saturdays. That’s why!\n",
      "One actual human appears in this movie and it’s a woman who throws a cat in the trash\n",
      "@gaepol Thanks!\n",
      "@__melbae What is that\n",
      "@alanalevinson The energy I’m trying to see!!!!!\n",
      "@NoelEWilliams 🔥🔥🔥\n",
      "@DaveDuFourNBA Is this a bit?\n",
      "not sure why DON’T F*CK WITH CATS isn’t just a straight up desktop (nonfiction) film. it would probably be more interesting, since it’s sort of halfway there anyways.\n",
      "@amandamull For a long ride I go straight for 4\n",
      "@Nelala_ Thank you my love, sound sooo much better! ❤️\n",
      "@bourtneyburton Thank you my love, hope 2020 is treating you well already! ❤️\n",
      "@LiquidTLO It's our favorite breakfast place here. So so good.\n",
      "@JimLaPorta @TSpoonFeed @BrandyZadrozny @yashar @pamelacolloff @MollyJongFast You’re right, you’re right. And you never, ever need to apologize to me ☺️🦋\n",
      "@Manny_Alicandro ...for five minutes! This lady was clearing out one group for the next one.\n",
      "@Taken_aBlack That Oscar Isaac photo! 😍\n",
      "@JamarrBrown #QuickPicsList cc: @MissBeaE\n",
      "@MargalitEwart Who dat in the corner 🤔😏😁\n",
      "@woahitsjuanito I’m just telling you as it is, fellow Aries 🔥😉\n",
      "speediest recovery wins ready set go\n",
      "@divadfoz At least he was moving. And got better as game wore on. I love Tanguy  but starting to worry he’s not built for this.\n",
      "@britnidlc it really is!!!\n",
      "@MikeDSykes Thought it was to stop Giannis crazy part is they’ll probably get to the conference finals regardless\n",
      "@conz Ha same, but it freaked me out at the time, band seemed unnerved by it\n",
      "i did that whole *drop chief keef - earned it at 11:56:47pm so the beat go off at midnight* and nobody got it\n",
      "Guardiola wanted a keeper who was good with his feet...\n",
      "@alec_sturm Happy New Year fam\n",
      "@PascalGibBN 😂😂😂\n",
      "@anne2themax 😭😭😭\n",
      "@jonquilynhill JQ 😂😂😂\n",
      "@scumbelievable “My ignorance is as valuable as your knowledge”\n",
      "Just had my first petty thought of 2020. Happy New Year!\n",
      "@HTMLflowers It’s the bleakest the show got\n",
      "INTO THE DARK: MIDNIGHT KISS: Closet monster. Impressively nails the specificity of a particular kind of group of affluent white gay men who are not especially interesting, but who think they’re interesting by virtue of their gayness. Very post-marriage equality horror. B+\n",
      "@peteeee Tell all the people I went to high school with that youre sitting next to that I say hi\n",
      "@Grady 😂😂😂😂 how is that even possible\n",
      "@violanorth Thanks homie!\n",
      "@kellykeegs You get me\n",
      "@kbport714 Thanks!\n",
      "@PulpCereal Email is in my bio (although I’m a bit behind bc of the holidays).\n",
      "Emotionally sabotage yourself by looping Taylor Swift’s “New Year’s Day” all day challenge\n",
      "@ndresfm oh LES DIABOLIQUES is an amazing film\n",
      "There is enough room for everyone. The Fire Marshall lets us know when it’s at maximum capacity. Allow people to be excited and give them the support that you may not have gotten when you started.\n",
      "Naturally there will be people who will get a quick fix and then stop working out but planning for their demise, just so you can be a gym snob, is low.\n",
      "Remember when you first started seriously working out and how hard it was? Remember feeling like you wanted to quit or not having peers to support you because you “joined a cult?”\n",
      "It’s a new year and, yes, gyms will be crowded but instead of expressing your disdain for the eager crowds, through elitist posts and memes, be a little more encouraging.\n",
      "Y’all are telling me that a house fried this rice?!\n",
      "@SethDRothman HNY, Sir - looking forward to getting back in the saddle\n",
      "@rachsyme To finally write that proposal &amp; book 🙏\n",
      "@KristyPuchko @kimbermyers Instead of book club, supper club 🍽\n",
      "@PulpCereal We got to see it first!\n",
      "guess I’m watching INTO THE DARK: MIDNIGHT KISS.\n",
      "@WordToReece 😂😂\n",
      "@kimbermyers @KristyPuchko Oh, I definitely need to learn this skill\n",
      "the energy I’m bringing into this decade is that of a 9V battery.\n",
      "should I get into BTS this year\n",
      "@oeste The LRG tee is too powerful.\n",
      "@KerrPoints Can’t live south of our nation’s capital 😂\n",
      "@WayneSisk1 You sound like me!\n",
      "@Skeeterstandup @FlappersComedy I love it there.\n",
      "@desertowl_13 Same.\n",
      "@DanielAshley13 Good food?\n",
      "@BitchieBootie Jelly new year! (Autocorrect but I'm keeping it)\n",
      "@PamHutch Thank you!\n",
      "@alissamarie @maddiewhittle I can also vouch for this. It's a great time\n",
      "@PamHutch Happy new year! I'm reading a book now and binge watching\n",
      "@InHollywoodland Happy new year!\n",
      "@Nomad313 Haha\n",
      "@jaclynf It felt very, very good to say this. 😀\n",
      "@smallnartless you already are\n",
      "Aidy Bryant about to takeover with The Shrill\n",
      "#coys\n",
      "@bwayducks @JoshLamon @QueenLesli @MaxCrumm @TheLucasSteele @annharada @TheNatalieWeiss Yesssssss! See you next weekend at The Duke!\n",
      "Además explica, a la luz de investigaciones académicas por qué los salvadoreños prefieron quedarse en los suburbios en lugar de #NYC.\n",
      "La información histórica describe las etapas de la inmigración a #NuevaYork y cómo empiezan a revelarse los datos de los Centroamericanos en los siglos XIX y XX.\n",
      "Jerk chicken from 2019. God's plan\n",
      "But when the distinctions between groups are so fine and largely based on institutional or political alignment rather than intellectual tradition, the argument can end up quite absurd on its merits.\n",
      "These two functions are not so related as one might think. People in a tradition of thought have lived through a variety of different political circumstances and alignments, possibly mixing factions around quite dramatically.\n",
      "Many political ideologies are obsessed with niche labels but none, it seems to me, more so than libertarians. The mileage they apparently expect to get from gerrymandering some new label just so to include only those they agree with most is extraordinary.\n",
      "@sales_off_film @Flixwise ❤️❤️❤️\n",
      "Pixar is making an isekai send tweet\n",
      "@KenParille You’re crazy for this one\n",
      "For*\n",
      "went to hell &amp; back last year . So grateful for the high notes but realistically dealt with so many low times. I pushed through despite the odds as I always do. 🙏🏽  I learned that not everyone has the same heart as me. I thank God for seeing me through. So ready fo 2020💚\n",
      "@WordToReece Migo can’t wait to meet his cousin!\n",
      "@frynaomifry He loved the slither of a sexy lady snake\n",
      "Here’s to an end to white supremacist patriarchy and dismantling all the layers of bystanders that enable its bullshit. Happy New Year. 🎉\n",
      "“You realize,” I said, “that your friend is the embodiment of white dude entitlement that represents everything that’s wrong with everything right now, right?”  He laughed and said, “Oh, he’s not entitled, he’s just—“\n",
      "(See aforementioned “who knows what kind of dude this one is” thought above.) He tried for a second more to convince us, and I decided to turn back to the friend, who was now talking to a woman.\n",
      "That grabs one of us?) and gathered our stuff to go.   His friend, decidedly more sober, came over and tried to explain that his friend was “cool.” I said, “He is most certainly not ‘cool.’” And I started to walk away.\n",
      "He said, “that’s ok.” He was sitting on our other friend’s coat. I said, “Dude. You gotta go.” He said, “ehhnnnnn” in his drunken slur. We were getting ready to go anyways, so, doing what women are trained to do, we avoided confrontation (would this be the one that swings at us?\n",
      "At the end of our usual delightful NYE night out (homie dinner + karaoke), a dude came up and plopped down in the seat, square in the middle of us, that had just been vacated by one of our people going over to the bathroom. I said, “Dude, someone’s sitting there.”\n",
      "@Millsy11374 Very here for this\n",
      "@ymercado @DanaSchwartzzz Only correct answer\n",
      "@DanaSchwartzzz Logan Marshall Greene\n",
      "Bring me this coat as if its an exquisite sacrifice\n",
      "rewatching THE HUDSUCKER PROXY (natch), and Tim Robbins is so adorable!!! So bright eyed and bushy tailed.\n",
      "@NoelMu Too cute! Happy New Year to you and the family. ❤️\n",
      "@karenkho 😞\n",
      "@ActaBunniFooFoo Anytime dude!! We have to play Halo together some time :)\n",
      "@Chrissao_ You’re already a step ahead there. Talking about it is a great place to start and I have confidence that you’ll achieve that goal!\n",
      "Rihanna Album/Fiona Album 2020\n",
      "Just remember guys… it’s 2020. We are lucky enough to have apps like @uber and @Lyft. Please, please, PLEASE don’t drink and drive! Enjoy yourselves tonight, but please keep yourself (and others) safe!\n",
      "@colindickey Champagne saberer strikes again!🥂\n",
      "What’s your New Years resolution?   For myself: I want to continue to focus on improving my commentary and hosting work. I also want to actually start getting serious about content creation.\n",
      "@KEBroady Sho ain’t\n",
      "@gareth_vader @JupiterPirates Oh thank you. Would love to catch up with Farnay one day.\n",
      "@DaltonReed5 Sehr gut! Deutsch lernen macht Spaß ☺️\n",
      "new year, new attempt to correctly write the date on invoices and checks\n",
      "Welcome to 2020\n",
      "Oh and the 2019 Mideast peace deal rollout was a success. עוד נצחון כזה...\n",
      "@KrisLovesMovies 💜💜💜💜\n",
      "@MediaversityRev 😂🤣\n",
      "@EriktheMovieman And to you as well! ☺️\n",
      "Happy Year That Cats Didn’t Come Out\n",
      "@sundownmotel It certainly has the best scenes of war among the stars\n",
      "Happy new year! #2020NewYear\n",
      "And if you want to ensure it’s an amazing one - go support @swingleft right now.\n",
      "Hot Girl ‘20s\n",
      "@AcrossTheArch Can’t wait to see you next week at The Duke!\n",
      "@xraylegend @SideshowGaming Felix Año to you too!!!\n",
      "Next New Years is either going to be amazing or absolutely horrific. There is no in between.   Have a happy 2020 all!\n",
      "@MykeCole You are the actual best.\n",
      "Happy new year, very thankful for all the sponsored content I integrated with this year\n",
      "Happy New Year to those who observe\n",
      "@sara_del sara same!!!!!!\n",
      "2019 was a great year, can’t wait to see what 2020 has in store for us all. Happy New Years ❤️\n",
      "DJ playing an aggressive Young Money set right now you have to respect it.\n",
      "YALL BEYONCE JUST POSTED A BLACK SQUARE WHAT DOES THIS MEANNNNNN\n",
      "Best decade of my life. Can’t complain. Onwards and better. #Happy2020\n",
      "@sara_del 💜💜💜💜💜💜💜\n",
      "2019 is probably the last year that America is a democracy. Anyway, yes, TELL HIM HOW YOU FEEL TONIGHT.\n",
      "@DPNash5 He’s always been...\n",
      "Wishing you all a happy, healthy, and prosperous #NewYear. I’m so thankful to all the amazing people in my life and can’t wait to see what 2020 has in store for us!  🎉💥🎉💥🎉\n",
      "@Evanwithani @bonesrubo Should worst his ass imho\n",
      "In conclusion, 2019 was a year of contrasts. Thank you.\n",
      "@HayesBrown @pixiedei Awww! Congrats!\n",
      "@heathcummingssr 🍻\n",
      "@YardsPerGretch 🍻\n",
      "Wishing everyone a very happy new year!\n",
      "@StelliniTweets Sterling Hitchcock\n",
      "@sarahcave76 @BernieSanders 🙌🏼\n",
      "Throughout the decade, I think I’ve really embraced who I am &amp; what I wanna be.  I’m sorta really different now but also... more me. If that makes sense to you, I think we’re doing it right.  ✌🏻2010s\n",
      "@janetmock @EW @Beyonce @tyraaross @MjRodriguez7 @IndyaMoore happy new year 🦄 🎉 😘\n",
      "@zzdoublezz YEP\n",
      "If you hit play on Irishman last Monday at noon, the final scene will end right as the clock strikes midnight. Start the new year off right\n",
      "@pwallinga The washest wash everything is a construct\n",
      "@Mercede73858625 Thank you\n",
      "@B0NNIE_ Bout to do the same in 2020.\n",
      "@bcsmith23 @jjscally_10 All the best for 2020, Brian - let’s do that 🙌\n",
      "@Jo_Unwin Ha yes, I have heard that one a few times in the last 34! Happy mutual bday, Jo 🎈\n",
      "@JLa_NYC Thanks Jenny 🙏\n",
      "@ideserveabeer 🤙\n",
      "@NYisBLUE Thank you my man, all the best to you and yours in 2020\n",
      "In fear of sounding hipsterish, crazy to think I've listened to @Skrillex consistently throughout the entire decade.  Remember a friend showing me \"My Name is Skrillex\" at my grandmother's house in 2010 and having good memories to \"Bangarang EP\" and \"Recess.\"\n",
      "@linzsports Just saying I think this year will be notable even compared to past precedent\n",
      "@linzsports That’s for sure\n",
      "@John_Hong Trying to theme my months, also tying a different exercise routine to each month\n",
      "@Kbald77 Let’s gooooooo\n",
      "@Politisite I’m still partying like it’s 1999.\n",
      "Is this...fan fiction??\n",
      "As a queer person of color, I will reiterate what I’ve said before: The only moral approach to 2020 is “Hope for the best; expect the worst.” Fight your battles accordingly. But do fight them.\n",
      "@akidos Dude congratulations on getting married!!!! 😍\n",
      "got a blister from playing videos games for 5 days straight\n",
      "@danieltkelley @lauriegale210 What a glow up 🙌\n",
      "@josephpallant Decade made :)\n",
      "@gletham Trur. It says a lot about the state of things and even more about the what CES thinks about women &amp; the industry.\n",
      "@YoAdriBaby 🙏🏻🙏🏻🙏🏻\n",
      "@hanwong LOL number one in my heart!\n",
      "@antbluejr 😂😂😂😂😂\n",
      "@thegnc You are freaking great.\n",
      "@josephpallant You are a good human.\n",
      "@LaDeziree ❤️❤️❤️❤️\n",
      "@zayasman Gracias! Thank you for the good wishes. Hope you have a wonderful NYE y Año Nuevo! ☺️\n",
      "Best songs lists are impossible, because there are like 140 top-10 songs from this decade, but those are the ones that game up from a quick perusal of my record collection and playlists.   It’s a perfect list.\n",
      "Go ahead and add “A More Perfect Union” by Titus Andronicus, “Hannah Hunt” by Vampire Weekend, “Paul” by Big Thief,” and “Elevator Operator” by Courtney Barnett and we’ll call it a top-10\n",
      "@HappytobeDee Sounds like a WHOLE plan!!\n",
      "Best songs of the 2010s, off the top of the old dome  1. Night Shift - Lucy Dacus  2. All Too Well - Taylor Swift 3. Me and My Dog - boygenius 4. Old Friends - Pinegrove 5. Ultralight Beam - Kanye West ft. Chance the Rapper  6. Your Best American Girl - Mitski\n",
      "@LenoreMariee 🥰 so so proud of YOU!\n",
      "@jorgevalens I thought you would like that one\n",
      "@Eugene_Scott 🙄\n",
      "@ERodriguez782 Ha, totally get it.Happy New Year to you.\n",
      "Moana was the best Disney movie of the decade, and I’m including everything in their IP portfolio\n",
      "happy bday to my fav fellow canadian @smrtdeath\n",
      "@DippyNikki My goodness that's impressive\n"
     ]
    }
   ],
   "source": [
    "result,cnt0,cnt1 = allInOne('Brooklyn_Week1.csv')\n",
    "nyc = pd.read_csv('Brooklyn_Week1.csv')\n",
    "for i in range(0,len(result)):\n",
    "    if(result[i]==1):\n",
    "        print(nyc['tweet'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2018-01-01\n2018-01-08\n2018-01-15\n2018-01-22\n2018-01-29\n2018-02-05\n2018-02-12\n2018-02-19\n2018-02-26\n2018-03-05\n2018-03-12\n2018-03-19\n2018-03-26\n2018-04-02\n2018-04-09\n2018-04-16\n2018-04-23\n2018-04-30\n2018-05-07\n2018-05-14\n2018-05-21\n2018-05-28\n2018-06-04\n2018-06-11\n2018-06-18\n2018-06-25\n2018-07-02\n2018-07-09\n2018-07-16\n2018-07-23\n2018-07-30\n2018-08-06\n2018-08-13\n2018-08-20\n2018-08-27\n2018-09-03\n2018-09-10\n2018-09-17\n2018-09-24\n2018-10-01\n2018-10-08\n2018-10-15\n2018-10-22\n2018-10-29\n2018-11-05\n2018-11-12\n2018-11-19\n2018-11-26\n2018-12-03\n2018-12-10\n2018-12-17\n2018-12-24\n2018-12-31\n"
     ]
    }
   ],
   "source": [
    "from datetime import date, timedelta\n",
    " \n",
    "year = 2018\n",
    "date_object = date(year, 1, 1)\n",
    "date_object += timedelta(days=1-date_object.isoweekday())\n",
    " \n",
    "while date_object.year == year:\n",
    "    print(date_object)\n",
    "    date_object += timedelta(days=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0                            is so sad for my APL frie...\n",
       "1                          I missed the New Moon trail...\n",
       "2                 .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "3                i think mi bf is cheating on me!!!   ...\n",
       "4                       or i just worry too much?        \n",
       "                              ...                        \n",
       "5282     waiting for claire to leave to glendale for t...\n",
       "5283     wanna do some sessions with ronnie he da bomb...\n",
       "5284          want my new mac! I'm getting impatient now!\n",
       "5285     wanted to see a Laker/Cav match-up. Oh well G...\n",
       "5286     wants to go on Ajax Experience 2009 http://bi...\n",
       "Name: tweet, Length: 5287, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('sadData.csv')\n",
    "df.loc[:,'tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "this week  5287 tweets collected in TimeSquare\nSad rate is 0.834909228441755\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[array([0, 0, 0, ..., 0, 0, 0]), 4415, 0.834909228441755]"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import re\n",
    "from string import punctuation\n",
    "import preprocessor as p\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "def preprocess(fileName):\n",
    "    nyc = pd.read_csv(fileName, encoding='ISO-8859-1')\n",
    "    for i in range(0, len(nyc)):\n",
    "        nyc.loc[:, 'tweet'][i] = p.clean(nyc.loc[:, 'tweet'][i])\n",
    "        nyc.loc[:, 'tweet'][i] = ''.join(\n",
    "            [c for c in nyc.loc[:, 'tweet'][i] if c not in punctuation])\n",
    "        nyc.loc[:, 'tweet'][i] = nyc.loc[:, 'tweet'][i].lower()\n",
    "    return nyc\n",
    "\n",
    "\n",
    "def predict(dataFrame, vec, lrModel):\n",
    "    nyc = dataFrame\n",
    "    tweet = vec.transform(nyc.loc[:, 'tweet'])\n",
    "    predict = lrModel.predict(tweet)\n",
    "    cnt0 = 0  # negative\n",
    "    cnt1 = 0  # positive\n",
    "    for i in range(0, len(nyc)):\n",
    "        if(predict[i] == 0):\n",
    "            cnt0 += 1\n",
    "        else:\n",
    "            cnt1 += 1\n",
    "    print(\"this week \", len(nyc), \"tweets collected in TimeSquare\")\n",
    "    sad_rate = cnt0/(len(nyc)+1)\n",
    "    print(\"Sad rate is\", sad_rate)\n",
    "    return [predict, cnt0, sad_rate]\n",
    "\n",
    "\n",
    "def allInOne(fileName):\n",
    "    nyc = preprocess(fileName)\n",
    "    lrModel = joblib.load('saved_model/lr.pkl')\n",
    "    vec = joblib.load('saved_model/vec.pkl')\n",
    "    print(nyc)\n",
    "    result = predict(nyc, vec, lrModel)\n",
    "\n",
    "    return result\n",
    "\n",
    "allInOne('sadData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}